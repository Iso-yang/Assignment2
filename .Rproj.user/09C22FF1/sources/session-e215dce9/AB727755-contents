---
title: "ETC3250/5250 Tutorial 9"
subtitle: "Support vector machines and regularisation"
author: "Prof. Di Cook"
date: "2024-04-29"
quarto-required: ">=1.3.0"
format:
    unilur-html:
        output-file: tutorial.html
        embed-resources: true
        css: "../assets/tutorial.css"
    unilur-html+solution:
        output-file: tutorialsol.html
        embed-resources: true
        css: "../assets/tutorial.css"
unilur-solution: true
---

<!--
devtools::install_github("koncina/unilur")
devtools::install_github("hadley/emo")

Had to run this in the week 9 folder in the terminal in R
cd C:\Users\jjew0003\Documents\Monash Teaching\ETC3250-5250\2025\week9
quarto install extension ginolhac/unilur

or 
cd C:\Users\jjew0003\Documents\Monash_Yr1\ETC3250\S12025\S12025\week9
quarto install extension ginolhac/unilur
-->

```{r echo=FALSE}
# Set up chunk for all slides
knitr::opts_chunk$set(
  fig.width = 4,
  fig.height = 4,
  fig.align = "center",
  out.width = "100%",
  code.line.numbers = FALSE,
  fig.retina = 3,
  echo = TRUE,
  message = FALSE,
  warning = FALSE,
  cache = FALSE,
  dev.args = list(pointsize = 11)
)
```

```{r}
#| echo: true
#| code-fold: true
#| code-summary: "Load the libraries and avoid conflicts, and prepare data"
# Load libraries used everywhere
library(tidyverse)
library(tidymodels)
library(patchwork)
library(mulgar)
library(tourr)
library(geozoo)
library(colorspace)
library(ggthemes)
library(conflicted)
conflicts_prefer(dplyr::filter)
conflicts_prefer(dplyr::select)
conflicts_prefer(dplyr::slice)
```

```{r}
#| echo: false
# Set plot theme
theme_set(theme_bw(base_size = 14) +
   theme(
     aspect.ratio = 1,
     plot.background = element_rect(fill = 'transparent', colour = NA),
     plot.title.position = "plot",
  
        plot.title = element_text(size = 24),
     panel.background = element_rect(fill = 'transparent', colour = NA),
     legend.background = element_rect(fill = 'transparent', colour = NA),
     legend.key = element_rect(fill = 'transparent', colour = NA)
   )
)

```

## `r emo::ji("target")` Objectives

The goal for this week is learn to about fitting support vector machine models. 

## `r emo::ji("wrench")` Preparation 

- Make sure you have all the necessary libraries installed. 

## Exercises: 

#### 1. A little algebra

Let $\mathbf{x_1}$ and $\mathbf{x_2}$ be vectors in $\mathbb{R}^2$, that is, two observations where $p=2$. By expanding $\mathcal{K}(\mathbf{x_1}, \mathbf{x_2}) = (1 + \langle \mathbf{x_1}, \mathbf{x_2}\rangle) ^2$ show that this is equivalent to an inner product of transformations of the original variables defined as $\mathbf{y} \in \mathbb{R}^6$.

<br>

Remember: $\langle \mathbf{x_1}, \mathbf{x_2}\rangle =\sum_{j=1}^{p} x_{1j}x_{2j}$.


::: unilur-solution
$$
\begin{align*}
\mathcal{K}(\mathbf{x_1}, \mathbf{x_2}) & = (1 + \langle \mathbf{x_1}, \mathbf{x_2}\rangle) ^2 \\
                                    & = \left(1 + \sum_{j = 1}^2 x_{1j}x_{2j} \right) ^2 \\
                                    & = (1 + x_{11}x_{21} + x_{12}x_{22})^2 \\
                                    & = (1 + x_{11}^2x_{21}^2 + x_{12}^2x_{22}^2 + 2x_{11}x_{21} + 2x_{12}x_{22} + 2x_{11}x_{12}x_{21}x_{22}) \\
                                    & = \langle \psi(\mathbf{x_1}), \psi(\mathbf{x_2}) \rangle
\end{align*}
$$

<br>
where $\mathbf{y} = (1, y_1^2, y_2^2, \sqrt{2}y_1, \sqrt{2}y_2, \sqrt{2}y_1y_2)$.

Try working it through in the reverse direction also.
:::

#### 2. Fitting and examining the support vector machine model

Simulate two toy data sets as follows. 

```{r}
#| code-summary: "Code to simulate data examples"
# Toy examples
set.seed(1125)
n1 <- 162
vc1 <- matrix(c(1, -0.7, -0.7, 1), ncol=2, byrow=TRUE)
#c1 <- rmvn(n=n1, p=2, mn=c(-2, -2), vc=vc1)
c1 <- rmvn(n=n1, p=2, mn=c(-1.5, -1.5), vc=vc1)
vc2 <- matrix(c(1, -0.4, -0.4, 1)*2, ncol=2, byrow=TRUE)
n2 <- 138
#c2 <- rmvn(n=n2, p=2, mn=c(2, 2), vc=vc2)
c2 <- rmvn(n=n2, p=2, mn=c(1.5, 1.5), vc=vc2)
df1 <- data.frame(x1=mulgar:::scale2(c(c1[,1], c2[,1])), 
                 x2=mulgar:::scale2(c(c1[,2], c2[,2])), 
                 cl = factor(c(rep("A", n1), 
                               rep("B", n2))))
#c1 <- sphere.hollow(p=2, n=n1)$points*3 + 
c1 <- sphere.hollow(p=2, n=n1)$points*1.2 + 
  c(rnorm(n1, sd=0.3), rnorm(n1, sd=0.3))
c2 <- sphere.solid.random(p=2, n=n2)$points
df2 <- data.frame(x1=mulgar:::scale2(c(c1[,1], c2[,1])), 
                  x2=mulgar:::scale2(c(c1[,2], c2[,2])), 
                  cl = factor(c(rep("A", n1), 
                               rep("B", n2))))
#红色点（类 A）：来自 c1，使用 sphere.hollow() 生成空心圆，并加上随机扰动。
#蓝色点（类 B）：来自 c2，使用 sphere.solid.random() 生成实心球体。


```

a. Make plots of each data set.

::: unilur-solution

```{r}
#| fig-width: 8
#| fig-height: 4
#| out-width: 100%
s1 <- ggplot() + 
  geom_point(data=df1, aes(x=x1, y=x2, colour=cl),
             shape=20) +
  scale_colour_discrete_divergingx(palette="Zissou 1") +
  theme(legend.position = "none") 

s2 <- ggplot() + 
  geom_point(data=df2, aes(x=x1, y=x2, colour=cl), shape=20) +
  scale_colour_discrete_divergingx(palette="Zissou 1") +
  theme(legend.position = "none") 

s1+s2+plot_layout(ncol=2)
```
红色点（类 A）：来自 c1，使用 sphere.hollow() 生成空心圆，并加上随机扰动。
蓝色点（类 B）：来自 c2，使用 sphere.solid.random() 生成实心球体。
类别 A (c1) 是一个空心球（环状），扰动之后变得稍微填满中心；
类别 B (c2) 是实心球，自然更集中在中心；
scale2() 标准化了变量，进一步让它们看起来分布更均匀。
所以你看到的图像结构是：内圈是密集的红点（类 B），外围散布着蓝点（类 A），这恰好体现了你用的球体生成逻辑。
:::

b. What type of kernel would be appropriate for each? How many support vectors would you expect are needed to define the boundary in each case?

::: unilur-solution

Linear and radial kernels would be recommended. 

We should be able to use as few as 3 points for the linear model. For the non-linear it would need many more points to define the circle boundary. 
:::

c. Break the data into training and test. 

::: unilur-solution

```{r}
set.seed(1141)
df1_split <- initial_split(df1, strata=cl)
df2_split <- initial_split(df2, strata=cl)
df1_tr <- training(df1_split)
df1_ts <- testing(df1_split)
df2_tr <- training(df2_split)
df2_ts <- testing(df2_split)
```
:::

d. Fit the linear svm model to the first data set. Try changing the cost parameter to explore the number of support vectors used and the size of the margin. You can use code like this (`scaled = FALSE` indicates that the variables are already scaled and no further standardising is needed):

```{r}
#| eval: false
svm_spec1 <- svm_linear(cost=1) |>
  set_mode("classification") |>
  set_engine("kernlab", scaled = FALSE)
svm_fit1 <- svm_spec1 |> 
  fit(cl ~ ., data = df1_tr)
svm_spec2 <- svm_rbf() |>
  set_mode("classification") |>
  set_engine("kernlab", scaled = FALSE)
svm_fit2 <- svm_spec2 |> 
  fit(cl ~ ., data = df2_tr)
```

::: unilur-solution

A small cost gives a large margin with severl supprot vectors

```{r}
svm_spec1 <- svm_linear(cost=0.5) |>
  set_mode("classification") |>
  set_engine("kernlab", scaled = FALSE)
svm_fit1 <- svm_spec1 |> 
  fit(cl ~ ., data = df1_tr)
svm_fit1 |>
  extract_fit_engine() |>
  plot(data=df1_tr)

```

A larger cost gives a smaller margin with fewer supprot vectors

```{r}
svm_spec1 <- svm_linear(cost=5) |>
  set_mode("classification") |>
  set_engine("kernlab", scaled = FALSE)
svm_fit1 <- svm_spec1 |> 
  fit(cl ~ ., data = df1_tr)
svm_fit1 |>
  extract_fit_engine() |>
  plot(data=df1_tr)

```

:::

e. Can you use the parameter estimates to write out the equation of the separating hyperplane for the linear SVM model? You can use `svm_fit1$fit@coef` and `svm_fit1$fit@SVindex` to  compute the coefficients as given by equation on slide 6 of week 8 slides. Try sketching your line on your plot.

::: unilur-solution

```{r}
sv_betas <- apply(svm_fit1$fit@coef*
                    df1_tr[svm_fit1$fit@SVindex,-3], 2, sum)
```
 
The equation of the separating hyperplane would be `r round(svm_fit1$fit@b, 2)` $+$ `r round(sv_betas[1], 2)`$x_1$ $+$ `r round(sv_betas[2], 2)`$x_2 = 0$.

```{r}
#| fig-width: 4
#| fig-height: 4
#| out-width: 50%
s1 + geom_abline(intercept=svm_fit1$fit@b, slope = -sv_betas[1]/sv_betas[2])
```
:::


f. Fit a non-linear svm model with the rbf kernel to the second data set. Try changing the cost parameter and kernel parameter sigma to explore the shape of the non-linear classifier. You can use code like this (`scaled = FALSE` indicates that the variables are already scaled and no further standardising is needed):

```{r}
#| eval: false
svm_spec2 <- svm_rbf(cost=1, rbf_sigma = 1) |>
  set_mode("classification") |>
  set_engine("kernlab", scaled = FALSE)
svm_fit2 <- svm_spec2 |> 
  fit(cl ~ ., data = df2_tr)
# svm_rbf(cost = 1, rbf_sigma = 1)：指定使用 RBF kernel（高斯径向基核） 的 SVM 模型；
# cost = 1：控制错误惩罚项（C 参数），越大对误差越不容忍；
# rbf_sigma = 1：控制 RBF 核函数中 σ 的大小，值越小表示决策边界越复杂。
# set_mode("classification")：表示我们是做分类任务（而不是回归）。
# set_engine("kernlab", scaled = FALSE)：指定用 kernlab 包中的 ksvm() 函数来训练模型；
# scaled = FALSE 表示不自动标准化特征变量（你已经手动用 scale2() 做过了，所以关闭自动缩放是合理的）。
```

::: unilur-solution


```{r}
svm_spec2 <- svm_rbf(cost=2, rbf_sigma = 0.5) |>
  set_mode("classification") |>
  set_engine("kernlab", scaled = FALSE)
svm_fit2 <- svm_spec2 |> 
  fit(cl ~ ., data = df2_tr) 

svm_fit2 |>
  extract_fit_engine() |>
  plot(data=df2_tr)
```
空心符号（○、△）：这些通常表示非支持向量（模型中“普通”的点）；
实心符号（●、▲）：这些通常表示支持向量（support vectors） —— 是最接近决策边界、对分类起关键作用的点
```{r}
svm_spec2 <- svm_rbf(cost=2, rbf_sigma = 5) |>
  set_mode("classification") |>
  set_engine("kernlab", scaled = FALSE)
svm_fit2 <- svm_spec2 |> 
  fit(cl ~ ., data = df2_tr) 

svm_fit2 |>
  extract_fit_engine() |>
  plot(data=df2_tr)
```

For fixed cost the larger value of the `rbf_sigma` fits a smoother non-linear classiifer.

:::






g. Compute the confusion table, and the test error for each model.

::: unilur-solution

```{r}
df1_pts <- df1_ts |>
  mutate(pcl = predict(svm_fit1, df1_ts)$.pred_class)
accuracy(df1_pts, cl, pcl)
df1_pts |>
  count(cl, pcl) |>
  group_by(cl) |>
  mutate(accuracy = n[cl==pcl]/sum(n)) |>
  pivot_wider(names_from = "pcl", 
              values_from = n, values_fill = 0) |>
  select(cl, A, B, accuracy)

df2_pts <- df2_ts |>
  mutate(pcl = predict(svm_fit2, df2_ts)$.pred_class)
accuracy(df2_pts, cl, pcl)
df2_pts <- df2_ts |>
  mutate(pcl = predict(svm_fit2, df2_ts)$.pred_class)
accuracy(df2_pts, cl, pcl)
df2_pts |>
  count(cl, pcl) |>
  group_by(cl) |>
  mutate(accuracy = n[cl==pcl]/sum(n)) |>
  pivot_wider(names_from = "pcl", 
              values_from = n, values_fill = 0) |>
  select(cl, A, B, accuracy)
```
:::

h. Which observations would you expect to be the support vectors? Overlay indications of the support vectors from each model to check whether the model thinks the same as you.

::: unilur-solution
```{r}
#| fig-width: 8
#| fig-height: 4
#| out-width: 100%
s1 <- s1 +
  geom_point(data=df1_tr[svm_fit1$fit@SVindex,], 
             aes(x=x1, y=x2, colour=cl),
             shape=1, size=4) + 
  geom_abline(
  intercept=svm_fit1$fit@b, 
  slope = -svm_fit1$fit@coef[[1]][1]/svm_fit1$fit@coef[[1]][2]) 
s2 <- s2 +   
  geom_point(data=df2_tr[svm_fit2$fit@SVindex,], 
             aes(x=x1, y=x2, colour=cl),
             shape=1, size=4)  
s1 + s2 + plot_layout(ncol=2)
# s1 显示线性 SVM 的支持向量和超平面
# s2 显示 RBF SVM 的支持向量（无直线）
# 都用了不同颜色表示分类
# 空心大圆圈高亮了对模型起决定作用的支持向量
```
🔸 功能：在图 s1 上添加支持向量点
🔸 df1_tr[svm_fit1$fit@SVindex, ]：提取训练集 df1_tr 中的支持向量（由索引表示）
🔸 shape=1：空心圆圈
🔸 size=4：点的大小为4
🔸 colour=cl：按分类变量着色
🔸 添加分类超平面（只适用于线性 SVM）
🔸 intercept = b：超平面的偏置项
🔸 slope = -w1/w2：超平面的斜率，来自超平面方程 w1*x1 + w2*x2 + b = 0
 ⟶ 转换为斜截式：x2 = -w1/w2 * x1 - b/w2
🔸 在图 s2 上也添加支持向量（空心圆）
🔸 同样方式提取索引，但不加直线（因为 RBF 是非线性模型，没有直线边界）


:::

i. Think about whether a neural network might be able to fit the second data set? Have a discussion with your neighbour and tutor about what would need to be done to make a neural network architecture.x

## `r emo::ji("wave")` Finishing up

Make sure you say thanks and good-bye to your tutor. This is a time to also report what you enjoyed and what you found difficult.
